{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31db7202",
   "metadata": {},
   "source": [
    "# Job Posting Data Analysis\n",
    "In this notebook, the group will be working with the [Job Posting in Singapore](https://www.kaggle.com/datasets/techsalerator/job-posting-data-in-singapore) dataset. This dataset will be used for processing, analyzing, and visualizing data.\n",
    "\n",
    "This project is carried out by the group **DS NERDS**, under Section **S19**, which consists of the following members:\n",
    "- Colobong, Franz Andrick\n",
    "- Chu, Andre Benedict M. \n",
    "- Pineda, Mark Gabriel A.\n",
    "- Rocha, Angelo H. \n",
    "  \n",
    "The output fulfills a part of the requirements for the course Statistical Modeling and Simulation (CSMODEL). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8457d5ab",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d865cde4",
   "metadata": {},
   "source": [
    "**TO-DO**:\n",
    "Put a brief description for each module used and how it was used in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dad24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ec2063",
   "metadata": {},
   "source": [
    "## Dataset Description and Collection Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b55163",
   "metadata": {},
   "source": [
    "This dataset offers a comprehensive overview of job openings across various sectors in Singapore. It provides an essential resource for businesses, job seekers, and labor market analysts, and it can also be a valuable tool for people who would like to be informed about job openings and employment trends in Singapore.\n",
    "\n",
    "The data was collected by a global data provider called **Techsalerator**, by consolidating and categorizing job-related information from diverse sources, including company websites, job boards, and recruitment agencies. \n",
    "\n",
    "Now, let us load the CSV file into our workspace with **'latin1'** encoding as it contains special characters (e.g., é, ñ, ’) that caused a UnicodeDecodeError with the default **'utf-8'** encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3505f99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_posting_df = pd.read_csv('Job Posting.csv', encoding='latin1')\n",
    "job_posting_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b035ce97",
   "metadata": {},
   "source": [
    "## Structure of the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1961dc90",
   "metadata": {},
   "source": [
    "**`TO-DO`** : Provide the following:\n",
    "- What each row and column represents (if tabular data)\n",
    "- Number of observations\n",
    "- What attributes or features are present in each observation\n",
    "\n",
    "** use the .info() function to visualize the answer to the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3bbde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_posting_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1d69f9",
   "metadata": {},
   "source": [
    "## Potential Implications of the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820536ac",
   "metadata": {},
   "source": [
    "**`TO-DO`** : Provide the potential implications of how the data was collected on the insights that will be generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0f36e1",
   "metadata": {},
   "source": [
    "## Key Data Fields "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5679b75",
   "metadata": {},
   "source": [
    "This section provides a brief description of the key attributes present in the dataset:\n",
    "\n",
    "\n",
    "- **Job Posting Date**: Captures the date a job is listed. This is crucial for job seekers and HR professionals to stay updated on the latest opportunities and trends.\n",
    "\n",
    "- **Job Title**: Specifies the position being advertised. This helps in categorizing and filtering job openings based on industry roles and career interests.\n",
    "\n",
    "- **Company Name**: Lists the hiring company. This information assists job seekers in targeting their applications and helps businesses track competitors and market trends.\n",
    "\n",
    "- **Job Location**: Provides the job's geographic location within Singapore. Job seekers use this to find opportunities in specific areas, while employers analyze regional talent and market conditions.\n",
    "\n",
    "- **Job Description**: Includes details about responsibilities, required qualifications, and other relevant aspects. This is vital for candidates to determine if they meet the requirements and for recruiters to communicate expectations clearly.\n",
    "\n",
    "**`TO-DO`** : Add the description for other important fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890b57fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_data_fields = job_posting_df[['First Seen At', 'Job Opening Title', 'Job Opening URL', 'Location', 'Description']]\n",
    "key_data_fields.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1c0c20",
   "metadata": {},
   "source": [
    "## Data Pre-Processing\n",
    "\n",
    "Before diving into analysis, it's essential to **clean and standardize** the dataset to ensure accurate insights. In this section, we focus on preparing the data by addressing duplicates, handling missing values, correcting data types, and other data pre-processing techniques to improve data quality and ensure consistency throughout the analysis.\n",
    "\n",
    "These steps help improve the **quality, reliability, and interpretability** of our exploratory data analysis (EDA) results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf030d8d",
   "metadata": {},
   "source": [
    "### Remove Unnecessary Columns\n",
    "Upon inspection, we can see that the **`Ticker`** column—referring to the stock ticker symbol of the company that posted the job—contains only null values.\n",
    "\n",
    "Since this column provides no usable information for analysis or modeling, we can safely drop it from the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203268a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the Ticker column\n",
    "null_count = job_posting_df['Ticker'].isna().sum()\n",
    "print(\"Unique Values:\", job_posting_df['Ticker'].unique())\n",
    "print(f\"Number of null values: {null_count}\")\n",
    "\n",
    "# Drop the column\n",
    "job_posting_df = job_posting_df.drop(columns=['Ticker'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa3e31f",
   "metadata": {},
   "source": [
    "### Remove Duplicate Job Postings\n",
    "\n",
    "Duplicate job postings can occur when the same job is scraped or recorded multiple times. If left unaddressed, these duplicates can **bias statistical insights**, such as job availability by location or contract type. Removing them ensures each job posting is only counted once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544eb255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Duplicates\n",
    "job_posting_df = job_posting_df.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7aa202",
   "metadata": {},
   "source": [
    "### Remove Entries with Missing Critical Information\n",
    "\n",
    "For the purpose of meaningful analysis, we remove records that lack crucial details such as:\n",
    "\n",
    "- **Location** and **Location Data**: Essential for analyzing geographic trends.\n",
    "- **Contract Type**: Helps determine the nature of the job, which is important for\n",
    "categorizing roles.\n",
    "- **Seniority**: Provides insight into job levels, which is useful for experience-based segmentation.\n",
    "- **O\\*NET Family**: Offers a standardized occupational classification, more reliable and structured than a free-text category.\n",
    "\n",
    "\n",
    "Missing any of these fields makes the data point less useful and **hinders the development of valid hypotheses** during exploratory data analysis (EDA).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c28170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of missing values in critical fields\n",
    "print(job_posting_df[['Location', 'Location Data', 'Contract Types', 'Seniority', 'O*NET Family']].isnull().sum())\n",
    "\n",
    "# Show how many entries are missing any of the five critical fields\n",
    "print(\n",
    "    \"Entries missing any of the critical fields:\",\n",
    "    job_posting_df[['Location', 'Location Data', 'Contract Types', 'Seniority', 'O*NET Family']]\n",
    "    .isnull().any(axis=1).sum(),\n",
    "    \"\\n\"\n",
    ")\n",
    "\n",
    "# Drop rows with any missing value in the critical columns\n",
    "job_posting_df = job_posting_df.dropna(\n",
    "    subset=['Location', 'Location Data', 'Contract Types', 'Seniority', 'O*NET Family'],\n",
    "    how='any'\n",
    ")\n",
    "\n",
    "# Re-check missing values after dropping\n",
    "print(job_posting_df[['Location', 'Location Data', 'Contract Types', 'Seniority', 'O*NET Family']].isnull().sum())\n",
    "print(\n",
    "    \"Entries missing any of the critical fields after cleaning:\",\n",
    "    job_posting_df[['Location', 'Location Data', 'Contract Types', 'Seniority', 'O*NET Family']]\n",
    "    .isnull().any(axis=1).sum()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30473970",
   "metadata": {},
   "source": [
    "### Fixing Incorrect Datatypes\n",
    "\n",
    "To ensure that each column is using the **appropriate data type**, we begin by inspecting the current data types of all fields:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b793b601",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_posting_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4407e368",
   "metadata": {},
   "source": [
    "### Convert Date-based Columns to DateTime Format\n",
    "\n",
    "The **date fields** in the dataset are initially represented as generic `object` types. In this step, we convert them to their appropriate `datetime` data types and ensure they are consistently formatted.\n",
    "\n",
    "To achieve this, we use the `pd.to_datetime()` method to parse each column. This allows us to catch any inconsistencies or formatting issues.\n",
    "\n",
    "Any values that fail to convert (e.g., due to invalid formats or corrupted entries) are automatically set to `NaT` (*Not a Time*), enabling us to easily identify and count invalid or missing entries per column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14719175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the date columns to check\n",
    "date_fields = ['First Seen At', 'Last Seen At', 'Job Last Processed At']\n",
    "date_df = job_posting_df[date_fields].copy()\n",
    "\n",
    "# Convert in-place and count invalid values\n",
    "for col in date_fields:\n",
    "    job_posting_df[col] = pd.to_datetime(job_posting_df[col], errors='coerce')\n",
    "    invalid_count = job_posting_df[col].isna().sum()\n",
    "    print(f\"{invalid_count:,} invalid date value(s) found in '{col}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e646028",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_posting_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad12df2b",
   "metadata": {},
   "source": [
    "### Standardize Text Fields\n",
    "\n",
    "To ensure consistency and simplify categorization, we clean key text fields by:\n",
    "\n",
    "- **Removing leading and trailing spaces**\n",
    "- **Converting all text to lowercase**\n",
    "\n",
    "This helps avoid mismatches due to inconsistent casing (e.g., `\"Full-Time\"` vs `\"full-time\"`) or trailing whitespace (`\"remote \"` vs `\"remote\"`), especially when grouping or filtering values in analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f02dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and standardize text-based columns\n",
    "job_posting_df['O*NET Family'] = job_posting_df['O*NET Family'].str.strip().str.lower()\n",
    "job_posting_df['Keywords'] = job_posting_df['Keywords'].str.strip().str.lower()\n",
    "job_posting_df['Location'] = job_posting_df['Location'].str.strip().str.lower()\n",
    "job_posting_df['Seniority'] = job_posting_df['Seniority'].str.strip().str.lower()\n",
    "job_posting_df['Contract Types'] = job_posting_df['Contract Types'].str.strip().str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99d803a",
   "metadata": {},
   "source": [
    "Now that all fields have been standardized to **consistent data types and formats**, we can proceed to the next step of the data pre-processing pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d24d9e0",
   "metadata": {},
   "source": [
    "## Categorizing Data for Simplified Analysis\n",
    "\n",
    "Many fields in the dataset, such as **`Seniority`**, **`Job Category`**, **`Location`**, **`Contract Types`**, and **`Skills`**, contain a wide variety of raw or inconsistent values. While these detailed values may be useful in certain cases, they can make analysis more difficult and less interpretable at a higher level.\n",
    "\n",
    "To address this, we apply **categorization and grouping techniques** to simplify the data. By consolidating similar or related values into broader, standardized categories, we can make comparisons and aggregations more meaningful\n",
    "\n",
    "As part of this process, we will import a custom module called `mapper.py`, which contains predefined functions for mapping and standardizing the values in each relevant column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2989253f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mapper as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51966614",
   "metadata": {},
   "source": [
    "### Categorizing Data by Seniority\n",
    "\n",
    "To simplify analysis based on job hierarchy, we categorize the `Seniority` field into broader groups. This allows us to analyze trends more effectively across different levels of responsibility.\n",
    "\n",
    "We begin by inspecting the unique values in the `Seniority` column and then apply a mapping to group them into three categories:\n",
    "- **Non-Managerial Position**\n",
    "- **Managerial Position**\n",
    "- **Executive Position**\n",
    "\n",
    "To understand the mapping dictionary for seniority, refer to the `mapper.py` file, where it is defined under the variable `seniority_mapping`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26f22f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all unique values\n",
    "unique_values_seniority = job_posting_df['Seniority'].unique()\n",
    "print(unique_values_seniority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772243b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the values and count categories\n",
    "seniority_categories = job_posting_df['Seniority'].map(mp.seniority_mapping)\n",
    "seniority_category_counts = seniority_categories.value_counts().sort_index()\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(seniority_category_counts)\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a64f077",
   "metadata": {},
   "source": [
    "### Categorizing Data by Job Field\n",
    "\n",
    "The **job field** was categorized by analyzing the contents of the `O*NET Family` column, which provides more specific insights into the required skills, education, and training for each role. Compared to the `Category` column—which offers a broader and often more general classification—the `O*NET Family` column is a more suitable choice for identifying and analyzing job fields with greater precision.\n",
    "\n",
    "The mapping logic for grouping related job fields is defined in the `mapper.py` file under the variable `job_fields_mappings`. Refer to that file to view how each O*NET Family is classified into a broader job field.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068960f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all unique values\n",
    "unique_job_fields = job_posting_df['O*NET Family'].unique()\n",
    "print(unique_job_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a831dff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the values and get the count of the categories\n",
    "job_fields_categories = job_posting_df['O*NET Family'].map(mp.job_fields_mapping)\n",
    "job_fields_category_counts = job_fields_categories.value_counts().sort_index()\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(job_fields_category_counts)\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06f874e",
   "metadata": {},
   "source": [
    "### Categorizing Data by Contract Types\n",
    "\n",
    "To streamline analysis of employment structures, we categorize the values in the `Contract Types` column. This involves identifying all unique contract types present in the dataset and mapping them to broader, standardized categories.\n",
    "\n",
    "The mapping logic is defined in the `mapper.py` file under the variable `contract_type_mapping`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9941644",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_contract_types = job_posting_df['Contract Types'].str.split(',').explode()\n",
    "\n",
    "unique_ctypes = split_contract_types.str.strip().unique()\n",
    "\n",
    "print(unique_ctypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dce0373",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Lowercase the contract types for case-sensitive look-ups, substring matching\n",
    "mapping_ctypes_lower = {k.lower(): v for k, v in mp.contract_types_mapping.items()}\n",
    "\n",
    "def map_ctypes_in_cell(str_keywords):\n",
    "    # check for null categories\n",
    "    if pd.isna(str_keywords):\n",
    "        return []\n",
    "\n",
    "    # list of contract types in lowercase\n",
    "    ctypes_lower = str_keywords.lower()\n",
    "    mapped_categories = []\n",
    "\n",
    "    # loop for the contract types inside the category\n",
    "    for ky, ct in mapping_ctypes_lower.items():\n",
    "        # if contract types is in the list, append to list the category\n",
    "        if ky in ctypes_lower:\n",
    "            mapped_categories.append(ct)\n",
    "\n",
    "    return mapped_categories\n",
    "\n",
    "all_ct = []\n",
    "\n",
    "# access every contract types in the dataset\n",
    "for ctypes_cells in job_posting_df['Contract Types']:\n",
    "\n",
    "    # get the category in the contract types cells then add/extend to the list\n",
    "    ct = map_ctypes_in_cell(ctypes_cells)\n",
    "    all_ct.extend(ct)\n",
    "\n",
    "# Example: cell has: ['full time', 'intern', 'long term] --> ['Full Time', 'Internship/Trainee', 'Long Term']  \n",
    "\n",
    "# counter for all categories\n",
    "ct_counts = Counter(all_ct) \n",
    "\n",
    "# transfer to a dataframe for better mapping\n",
    "df_counts = pd.DataFrame(list(ct_counts.items()), \n",
    "                        columns=['Contract Types', 'Count'])\n",
    "\n",
    "df_counts = df_counts.sort_values('Count', ascending=False)\n",
    "\n",
    "# Print the values\n",
    "df_counts = df_counts.reset_index(drop=True)\n",
    "print(\"=\" * 50)\n",
    "print(df_counts.to_string(index=False))\n",
    "print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcf0b79",
   "metadata": {},
   "source": [
    "### Categorizing Data by Keywords\n",
    "\n",
    "The `Keywords` column often contains multiple entries separated by commas. To ensure accurate grouping and analysis, we first clean and split these entries into individual keywords. This helps reduce redundancy caused by inconsistent formatting (e.g., extra spaces, mixed casing).\n",
    "\n",
    "After cleaning, we identify all unique keyword values and apply standardized categorization where needed. This step supports clearer interpretation of skillsets or role-related descriptors associated with each job posting.\n",
    "\n",
    "The logic for processing and mapping keyword values is handled in the `mapper.py` file under the relevant keyword-related functions or mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5f00f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the contents of the keywords column\n",
    "split_keywords = job_posting_df['Keywords'].str.split(',').explode()\n",
    "\n",
    "# Then find the unique values, these mitigates redundancy a lot\n",
    "unique_keywords = split_keywords.str.strip().unique()\n",
    "\n",
    "print(unique_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baba3f12",
   "metadata": {},
   "source": [
    "The `Keywords` column contained over **500+ unique values**, many of which represented similar or related concepts. To reduce redundancy and make analysis more manageable, the values were **grouped into broader categories** such as *Programming Languages*, *Frameworks & Libraries*, *Tools & Platforms*, and others.\n",
    "\n",
    "The mapping process was initially assisted by AI to generate a comprehensive list of groupings. A group member then manually **reviewed and verified** the mappings to ensure accuracy and consistency across the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbc81c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Lowercase the keys for case-sensitive look-ups, substring matching\n",
    "mapping_skills_lower = {k.lower(): v for k, v in mp.keywords_skills_mapping.items()}\n",
    "\n",
    "def map_keywords_in_cell(str_keywords):\n",
    "    # check for null categories\n",
    "    if pd.isna(str_keywords):\n",
    "        return []\n",
    "\n",
    "    # list of keywords in lowercase\n",
    "    keywords_lower = str_keywords.lower()\n",
    "    mapped_categories = []\n",
    "\n",
    "    # loop for the key inside the category\n",
    "    for ky, ct in mapping_skills_lower.items():\n",
    "        # if keyword is in the list, append to list the category\n",
    "        if ky in keywords_lower:\n",
    "            mapped_categories.append(ct)\n",
    "\n",
    "    return mapped_categories\n",
    "\n",
    "all_ct = []\n",
    "\n",
    "# access every keyword in the dataset\n",
    "for ky_cells in job_posting_df['Keywords']:\n",
    "\n",
    "    # get the category in the keywords cells then add/extend to the list\n",
    "    ct = map_keywords_in_cell(ky_cells)\n",
    "    all_ct.extend(ct)\n",
    "\n",
    "# Example: cell has: ['c++', 'mysql', 'linux] --> ['Programming Language', 'Databases', 'Operating System']  \n",
    "\n",
    "# counter for all categories\n",
    "ct_counts = Counter(all_ct) \n",
    "\n",
    "# transfer to a dataframe for better mapping\n",
    "df_counts = pd.DataFrame(list(ct_counts.items()), \n",
    "                        columns=['Category', 'Count'])\n",
    "\n",
    "df_counts = df_counts.sort_values('Count', ascending=False)\n",
    "\n",
    "# Print the values\n",
    "df_counts = df_counts.reset_index(drop=True)\n",
    "print(\"=\" * 50)\n",
    "print(df_counts.to_string(index=False))\n",
    "print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cafb37",
   "metadata": {},
   "source": [
    "### Parsing and Analyzing Location Data\n",
    "\n",
    "To extract meaningful insights from the `Location Data` column, we begin by parsing its contents—originally stored in JSON-like strings—into structured dictionaries. We begin by creating a copy of the main dataset into `locations_df` to avoid modifying the original `job_posting_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e9fc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_df = job_posting_df.copy()\n",
    "# Check the contents of the Location Data\n",
    "locations_df['Location Data']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a3f454",
   "metadata": {},
   "source": [
    "This section focuses on extracting structured information from the `Location Data` column, which contains location details in JSON format. Each entry is parsed into a Python dictionary using a custom `parse_location()` function. This function handles both single dictionary entries and lists of dictionaries, returning a standardized format for further processing.\n",
    "\n",
    "After parsing, the `json_normalize()` method is used to flatten the nested data structure, transforming the location attributes (such as `city`, `region`, and `country`) into separate columns. This results in a cleaner and more analyzable format, allowing us to explore geographic distributions—such as the number of job postings per country.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4282bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse data into a dictionary\n",
    "def parse_location(str_location):\n",
    "    try:\n",
    "        # Convert the json file into a python object\n",
    "        data = json.loads(str_location)\n",
    "\n",
    "        # Takes the first element: if a list, else returns the dictionary as the \n",
    "        # first element, otherwise return the dictionary\n",
    "        return data[0] if isinstance(data, list) else data\n",
    "    except:\n",
    "        # Return an empty list\n",
    "        return {}\n",
    "\n",
    "# Parse the location data  \n",
    "locations_df['Location Data'] = locations_df['Location Data'].apply(\n",
    "    parse_location\n",
    ")\n",
    "\n",
    "# Normalize Location Data into new columns and rows\n",
    "locations_df = pd.json_normalize(locations_df['Location Data'])\n",
    "locations_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b8cd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the contents by categorizing the total number of entries per country\n",
    "locations_df['country'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4386986",
   "metadata": {},
   "source": [
    "### Salary Data Extraction and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e26074",
   "metadata": {},
   "source": [
    "To better understand the **`Salary Data`** column, we begin by creating a separate copy of the original DataFrame called `salary_df`. This ensures that all salary-related transformations and cleaning steps can be performed safely without altering the original `job_posting_df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ac9797",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_df = job_posting_df.copy()\n",
    "salary_df['Salary Data']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fc4dcb",
   "metadata": {},
   "source": [
    "Upon inspection, we notice that the salary descriptions are stored as **JSON objects**—but currently in the form of **JSON strings**.\n",
    "\n",
    "To make this data usable, we will:\n",
    "\n",
    "1. **Parse** each string into a Python dictionary.\n",
    "2. **Normalize** the dictionary so that each key becomes its own separate column in the DataFrame.\n",
    "\n",
    "This will give us a clearer structure, allowing us to inspect and clean salary values more effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1651a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_df = job_posting_df.copy()\n",
    "\n",
    "# Parse json object into a dictionary\n",
    "salary_df['Salary Data'] = salary_df['Salary Data'].apply(\n",
    "    lambda x: json.loads(x) if isinstance(x, str) else x\n",
    ")\n",
    "\n",
    "# Normalize Salary Data into new columns and remove rows with null values\n",
    "salary_df = pd.json_normalize(salary_df['Salary Data'])\n",
    "salary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34342eb4",
   "metadata": {},
   "source": [
    "By running `salary_df.info()`, we can observe that out of thousands of job postings, only **434** entries contain salary-related information. \n",
    "\n",
    "Since salary is a critical detail when analyzing job data, we want to ensure our next steps focus only on entries where salary is provided. To simplify our cleaning process, we will **temporarily drop rows with null values** for salary-related fields.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fa07b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_df.info() \n",
    "\n",
    "# Drop rows with any null values\n",
    "salary_df.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e9b920",
   "metadata": {},
   "source": [
    "Now that we've removed rows with null values, we can inspect the unique values present in each field. \n",
    "\n",
    "In particular, the **`salary_currency`** column contains two distinct values: **USD** and **EUR**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463b5a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_df['salary_currency'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490e9d37",
   "metadata": {},
   "source": [
    "After checking the `salary_currency` field, we observe that most job salaries are already in **USD**. \n",
    "\n",
    "To ensure consistency in our analysis, we will normalize the data by converting all **EUR** salaries to **USD** using the exchange rate as of **June 17, 2025**:\n",
    "\n",
    "- **1 EUR = 1.15 USD**\n",
    "\n",
    "This conversion allows us to compare salaries more accurately and ensures uniformity across the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51bd33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define conversion rate from EUR to USD\n",
    "conversion_rate = 1.15\n",
    "\n",
    "# Convert EUR to USD\n",
    "for index, row in salary_df.iterrows():\n",
    "    if row['salary_currency'] == 'EUR':\n",
    "        salary_df.loc[index, 'salary_low'] = row['salary_low'] * conversion_rate\n",
    "        salary_df.loc[index, 'salary_high'] = row['salary_high'] * conversion_rate\n",
    "        salary_df.loc[index, 'salary_currency'] = 'USD'\n",
    "\n",
    "# Drop redundant salary column \n",
    "salary_df.drop(columns=['salary_low_usd', 'salary_high_usd'], inplace=True, errors='ignore')\n",
    "\n",
    "salary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131fa600",
   "metadata": {},
   "source": [
    "Now that all the salaries are represented in **USD**, we can focus on the `salary_time_unit` column, which is categorized into three values: **hour**, **month**, and **year**. These indicate how each salary is paid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e72249",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_df['salary_time_unit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adad4840",
   "metadata": {},
   "source": [
    "We notice that most salaries are already given on an **annual basis**. To maintain consistency and enable easier comparisons, we will convert all salaries to **annual salary**.\n",
    "\n",
    "#### Conversion Formulas:\n",
    "- **Monthly to Annual**:\n",
    "  - `annual_salary = monthly_salary * 12`\n",
    "\n",
    "- **Hourly to Annual** (assuming a standard 9-to-5 schedule):\n",
    "  - `hours_per_week = 40`\n",
    "  - `weeks_per_year = 52`\n",
    "  - `hourly_to_annual = 40 * 52 = 2080`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b542a2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion factors\n",
    "monthly_to_annual = 12\n",
    "hours_per_week = 40\n",
    "weeks_per_year = 52\n",
    "hourly_to_annual = hours_per_week * weeks_per_year  # 40 * 52 = 2080\n",
    "\n",
    "for index, row in salary_df.iterrows():\n",
    "    # Convert hourly salaries to annual\n",
    "    if (row['salary_time_unit'] == 'hour'):\n",
    "        salary_df.loc[index, 'salary_low'] = row['salary_low'] * hourly_to_annual\n",
    "        salary_df.loc[index, 'salary_high'] = row['salary_high'] * hourly_to_annual\n",
    "        salary_df.loc[index, 'salary_time_unit'] = 'year'\n",
    "    \n",
    "    # Convert monthly salaries to annual\n",
    "    elif (row['salary_time_unit'] == 'month'):\n",
    "        salary_df.loc[index, 'salary_low'] = row['salary_low'] * monthly_to_annual\n",
    "        salary_df.loc[index, 'salary_high'] = row['salary_high'] * monthly_to_annual\n",
    "        salary_df.loc[index, 'salary_time_unit'] = 'year'\n",
    "\n",
    "    # Retain annual salaries\n",
    "    else:\n",
    "        salary_df.loc[index, 'salary_low'] = row['salary_low']\n",
    "        salary_df.loc[index, 'salary_high'] = row['salary_high']\n",
    "\n",
    "salary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239f7568",
   "metadata": {},
   "source": [
    "Now that all salaries are in the same currency (**USD**) and time unit (**annual**), we can focus on the `salary_low` and `salary_high` fields.\n",
    "\n",
    "These two fields represent the **lower and upper bounds** of the offered salary range. To simplify the analysis and create a single representative salary value, we will take the **mean** of these two values.\n",
    "\n",
    "This gives us a new column, `annual_salary`, which reflects the average offered salary for the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206ebd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_df['annual_salary'] = (salary_df[['salary_low', 'salary_high']].mean(axis=1))\n",
    "salary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136f679c",
   "metadata": {},
   "source": [
    "Now that we've created the `annual_salary` column, the original fields—`salary_low`, `salary_high`, `salary_currency`, and `salary_time_unit`—are no longer needed for further analysis.\n",
    "\n",
    "To clean up the DataFrame and simplify its structure, we will drop these columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7489ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_df.drop(columns=['salary_low', 'salary_high', 'salary_currency', 'salary_time_unit'], inplace=True)\n",
    "salary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51408864",
   "metadata": {},
   "source": [
    "Now that we've cleaned and normalized the salary information into a single `annual_salary` column, we can integrate it back into the original `job_posting_df`.\n",
    "\n",
    "We will assign this as a new column called `Annual_Salary`, allowing us to analyze job postings alongside their corresponding annual salaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18af56d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the annual salary to the original job_posting_df\n",
    "job_posting_df['Annual_Salary'] = salary_df['annual_salary']\n",
    "job_posting_df[job_posting_df['Annual_Salary'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ccb96c",
   "metadata": {},
   "source": [
    "**`TO-DO`** : Check for outliers in the annual salary column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b2ba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for Outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb58972",
   "metadata": {},
   "source": [
    "## General Research Question\n",
    "\n",
    "Understanding the global job landscape is essential for identifying emerging opportunities, evolving skill demands, and industry-wide shifts across different regions. By analyzing job postings and employment data, we can uncover meaningful insights into how the workforce is transforming over time.\n",
    "\n",
    "With this in mind, the group formulated the research question:\n",
    "\n",
    "> **What are the underlying patterns and trends in the international job market?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1832b3",
   "metadata": {},
   "source": [
    "**`TO-DO`** : Answer the eda questions. If applicable, use data visualization tehcniques to better display the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774fcc47",
   "metadata": {},
   "source": [
    "### EDA Question 1 - Annual Salary and Job Field \n",
    "Job field in this case lies on their category within the `O*NET Family` categorization in the dataset. In this EDA question, the researchers aim to understand the following:\n",
    "- What is the relationship between the annual salary and the job field in the dataset?\n",
    "- What is the average salary for each job field?\n",
    "- Which job fields show the lowest and highest salary variability?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851916b8",
   "metadata": {},
   "source": [
    "### EDA Question 2 - Seniority and Contract Types (and Salary Relevance)\n",
    "The researchers aim for this EDA question are to identify related patterns and trends within the `Seniority` and `Contract Types` variables. They will be guided by the following questions:\n",
    "- What is the relationship between seniority and contract types in the dataset?\n",
    "- What is the salary distribution for each combination/category of seniority and their equal contract types?\n",
    "- Are certain contract types more prevalent at specific seniority levels?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c384ab",
   "metadata": {},
   "source": [
    "### EDA Question 3 - Locations and Skills\n",
    "Skills in this case lies on their category within the `Keywords` categorization in the dataset. In this EDA question, the researchers aim to understand the following:\n",
    "- What is the relationship between the skills required by companies that are outsourcing to specific locations?\n",
    "- What are the prevalent skill categories that exist for each location?\n",
    "- Which locations have the highest demand for specific skills?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
